{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/thechemist54/.local/lib/python3.10/site-packages (2.0.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post7.tar.gz (3.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /home/thechemist54/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/thechemist54/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/thechemist54/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/thechemist54/.local/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: click in /home/thechemist54/.local/lib/python3.10/site-packages (from nltk) (8.1.5)\n",
      "Requirement already satisfied: joblib in /home/thechemist54/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2023.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.4/770.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /home/thechemist54/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0.post7-py3-none-any.whl size=2360 sha256=536525d5eaacf6c19f1aebc7ac1fe617173ee41b7f0b24f54ada1ed4390102be\n",
      "  Stored in directory: /home/thechemist54/.cache/pip/wheels/c8/9c/85/72901eb50bc4bc6e3b2629378d172384ea3dfd19759c77fd2c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2023.6.3 sklearn-0.0.post7\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas sklearn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/thechemist54/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/thechemist54/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/thechemist54/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our unstructured data\n",
    "data = [\n",
    "    [\"To Kill a Mockingbird\", \"Harper Lee\", \"JohnDoe\", \"2023-06-23\", \"An unforgettable novel of a childhood in a sleepy Southern town and the crisis of conscience that rocked it\", 5],\n",
    "    [\"1984\", \"George Orwell\", \"JaneDoe\", \"2023-06-24\", \"A frighteningly prophetic novel about the power of government surveillance and public manipulation\", 4],\n",
    "    [\"Moby Dick\", \"Herman Melville\", \"SamDoe\", \"2023-06-25\", \"A seemingly endless, convoluted whaling voyage filled with intricate details about the sea... Not my cup of tea\", 2],\n",
    "    [\"Pride and Prejudice\", \"Jane Austen\", \"AnnDoe\", \"2023-06-26\", \"A classic novel that blends romance with wit, and social criticism. Loved it\", 5],\n",
    "    [\"The Great Gatsby\", \"F. Scott Fitzgerald\", \"RickDoe\", \"2023-06-27\", \"This novel provides an incredibly discerning glimpse into the American soul, but the characters felt somewhat hollow\", 3],\n",
    "    [\"War and Peace\", \"Leo Tolstoy\", \"EveDoe\", \"2023-06-28\", \"A grand narrative that combines history, philosophy, and keen insight into the human heart. A challenging yet rewarding read\", 4],\n",
    "    [\"Crime and Punishment\", \"Fyodor Dostoevsky\", \"TomDoe\", \"2023-06-29\", \"A brilliant exploration of the human psyche when faced with profound moral dilemmas. Dark but profound\", 4],\n",
    "    [\"Wuthering Heights\", \"Emily Bronte\", \"ZaraDoe\", \"2023-06-30\", \"A dramatic narrative filled with raw emotions. The gothic atmosphere was spellbinding, though the story was slightly melodramatic\", 3]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=[\"Title\", \"Author\", \"Reviewer\", \"Date\", \"Review\", \"Rating\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokenized_text = nltk.word_tokenize(text.lower())\n",
    "    cleaned_text = [lemmatizer.lemmatize(t) for t in tokenized_text if t not in stop_words and t.isalpha()]\n",
    "    return \" \".join(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cleaned_Review\"] = df[\"Review\"].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df[\"Cleaned_Review\"])\n",
    "y = df[\"Rating\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the matrix to a dense format\n",
    "dense_X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names (words)\n",
    "feature_names = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the dense matrix\n",
    "df_tfidf = pd.DataFrame(dense_X, columns=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   american  atmosphere     blend  brilliant  challenging  character   \n",
      "0  0.000000         0.0  0.000000        0.0          0.0   0.000000  \\\n",
      "1  0.000000         0.0  0.000000        0.0          0.0   0.000000   \n",
      "2  0.000000         0.0  0.000000        0.0          0.0   0.000000   \n",
      "3  0.000000         0.0  0.367556        0.0          0.0   0.000000   \n",
      "4  0.310056         0.0  0.000000        0.0          0.0   0.310056   \n",
      "\n",
      "   childhood   classic  combine  conscience  ...  story  surveillance   \n",
      "0   0.344991  0.000000      0.0    0.344991  ...    0.0      0.000000  \\\n",
      "1   0.000000  0.000000      0.0    0.000000  ...    0.0      0.367556   \n",
      "2   0.000000  0.000000      0.0    0.000000  ...    0.0      0.000000   \n",
      "3   0.000000  0.367556      0.0    0.000000  ...    0.0      0.000000   \n",
      "4   0.000000  0.000000      0.0    0.000000  ...    0.0      0.000000   \n",
      "\n",
      "        tea  though      town  unforgettable    voyage   whaling       wit   \n",
      "0  0.000000     0.0  0.344991       0.344991  0.000000  0.000000  0.000000  \\\n",
      "1  0.000000     0.0  0.000000       0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.305675     0.0  0.000000       0.000000  0.305675  0.305675  0.000000   \n",
      "3  0.000000     0.0  0.000000       0.000000  0.000000  0.000000  0.367556   \n",
      "4  0.000000     0.0  0.000000       0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "   yet  \n",
      "0  0.0  \n",
      "1  0.0  \n",
      "2  0.0  \n",
      "3  0.0  \n",
      "4  0.0  \n",
      "\n",
      "[5 rows x 75 columns]\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(df_tfidf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Confusion Matrix:\n",
      " [[0 0 0]\n",
      " [1 0 1]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction and evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-23 15:50:24.338225: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 15:50:24.575968: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-23 15:50:24.577425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-23 15:50:25.940405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ratings into a binary classification problem\n",
    "df['Rating'] = df['Rating'].apply(lambda x: 1 if x >= 4 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the text data to integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['Cleaned_Review'])\n",
    "encoded_X = encoder.transform(df['Cleaned_Review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integers to one-hot encoded data\n",
    "dummy_X = np_utils.to_categorical(encoded_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dummy_X, df['Rating'], test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(8, input_dim=dummy_X.shape[1], activation='relu'))\n",
    "model1.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 0.5628 - accuracy: 0.8333\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5610 - accuracy: 0.8333\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5592 - accuracy: 0.8333\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5574 - accuracy: 0.8333\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5556 - accuracy: 0.8333\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5539 - accuracy: 0.8333\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5521 - accuracy: 0.8333\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5503 - accuracy: 0.8333\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5485 - accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5468 - accuracy: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2c6c6216c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model1.fit(X_train, y_train, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 445ms/step - loss: 0.9838 - accuracy: 0.5000\n",
      "\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
